import cudf
import cugraph
import dask_cudf
from dask_cuda import LocalCUDACluster
from dask.distributed import Client
import cupy as cp
import json
import time
import numpy as np

def create_gpu_graph(file_path):
    """
    ëŒ€ê·œëª¨ ê¸ˆìœµ ê±°ë˜ ë„¤íŠ¸ì›Œí¬ë¥¼ GPUì—ì„œ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ (cuGraph í™œìš©)

    :param file_path: ë°ì´í„° íŒŒì¼ ê²½ë¡œ (.parquet ë˜ëŠ” .csv)
    :return: GPU ê¸°ë°˜ cuGraph ê·¸ë˜í”„ ê°ì²´ ë° ë¬¸ìì—´ ID ë§¤í•‘ ì •ë³´
    """

    # ğŸš€ ë°ì´í„° ë¡œë“œ (Dask ê¸°ë°˜ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬)
    if file_path.endswith(".parquet"):
        ddf = dask_cudf.read_parquet(file_path, split_out=10)  # ğŸ”¥ ë³‘ë ¬ ë¡œë”© ì¶”ê°€
    elif file_path.endswith(".csv"):
        ddf = dask_cudf.read_csv(file_path, dtype={"source": "str", "target": "str"}, split_out=10)
    else:
        raise ValueError("ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. CSV ë˜ëŠ” Parquetì„ ì‚¬ìš©í•˜ì„¸ìš”.")

    print(f"âœ… Data Loaded: {file_path}")

    # ğŸš€ í•„ìš”í•œ ì»¬ëŸ¼ ì„ íƒ (source, target, weight)
    ddf = ddf[["source", "target", "amount"]]

    # ğŸš€ ğŸ’¡ source, targetì„ int64ë¡œ ë³€í™˜ (Factorization)
    unique_nodes = cudf.concat([ddf["source"], ddf["target"]]).unique()
    node_map = cudf.Series(data=cp.arange(len(unique_nodes), dtype="int64"), index=unique_nodes)

    # ğŸš€ ë¬¸ìì—´ì„ ì •ìˆ˜ IDë¡œ ë§¤í•‘
    ddf["source"] = ddf["source"].map(node_map).astype("int64")
    ddf["target"] = ddf["target"].map(node_map).astype("int64")

    print("âœ… String IDs Converted to Int64 IDs")

    # ğŸš€ ì¤‘ë³µ ê°„ì„ ì„ ê·¸ë£¹í™”í•˜ì—¬ Weight ì²˜ë¦¬ (Multi-Edges â†’ Weighted Edge)
    ddf = ddf.groupby(["source", "target"]).agg({"amount": "sum"}).reset_index()

    # ğŸš€ GPU ê¸°ë°˜ cudf ë³€í™˜
    gdf = ddf.compute().to_cudf()
    print(f"âœ… Data Converted to cuDF (GPU)")

    # ğŸš€ cuGraph ë¬´ë°©í–¥ ê·¸ë˜í”„ ìƒì„±
    G = cugraph.Graph()
    G.from_cudf_edgelist(gdf, source="source", destination="target", edge_attr="amount")

    print(f"âœ… Graph Created with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges")
    return G, gdf, node_map


import cudf
import cugraph
import cupy as cp
import json
import time


def propagation_laundering_score(G, gdf, node_map, max_iter=10):
    """
    GPU ê¸°ë°˜ Louvain ì»¤ë®¤ë‹ˆí‹° íƒìƒ‰ ë° Label Propagation ìˆ˜í–‰

    :param G: cuGraph ê·¸ë˜í”„ ê°ì²´
    :param gdf: cuDF ë°ì´í„° í”„ë ˆì„ (source, target, amount)
    :param node_map: {str â†’ int64} ë³€í™˜ëœ ë…¸ë“œ ë§¤í•‘ ì •ë³´
    :param max_iter: Label Propagation ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜
    :return: laundering_score (ê° ë…¸ë“œì˜ ì›ë˜ ë¬¸ìì—´ ê°’ ë§¤í•‘ ê²°ê³¼)
    """

    # ğŸš€ 1ï¸âƒ£ Louvain ì»¤ë®¤ë‹ˆí‹° íƒìƒ‰ (GPU)
    start = time.time()
    communities = cugraph.louvain(G)
    end = time.time()
    print(f"âœ… Louvain Communities: {len(communities)} detected in {end - start:.5f} sec")

    # ğŸš€ 2ï¸âƒ£ ë…¸ë“œ IDë¥¼ ì›ë˜ `str` ê°’ìœ¼ë¡œ ë³€í™˜ (ì—­ë§¤í•‘)
    inverse_node_map = {v: k for k, v in node_map.to_pandas().items()}

    communities_dict = communities.to_pandas().set_index("vertex")["partition"].to_dict()
    communities_str = {inverse_node_map[node]: community for node, community in communities_dict.items()}

    # ğŸš€ 3ï¸âƒ£ Louvain ì»¤ë®¤ë‹ˆí‹° ê²°ê³¼ JSON ì €ì¥ (ë¬¸ìì—´ ë…¸ë“œ IDë¡œ ë³€í™˜)
    with open("../communities_cd_v0.1.json", "w") as f:
        json.dump(communities_str, f, indent=4)

    print(f"âœ… Saved Louvain Communities with Original Node Names")

    # ğŸš€ 4ï¸âƒ£ Score ì´ˆê¸°í™” (cuDF ì‚¬ìš©)
    laundering_score = cudf.Series(data=cp.zeros(G.number_of_nodes()), index=communities["vertex"].to_pandas())

    # ğŸš€ 5ï¸âƒ£ ìê¸ˆì„¸íƒ ê±°ë˜ ë…¸ë“œ ì´ˆê¸°ê°’ ì„¤ì •
    laundering_nodes = gdf[gdf["amount"] > 0]  # ê¸ˆì•¡ì´ ìˆëŠ” ê²½ìš°ë¡œ ê°€ì • (ë³€ê²½ ê°€ëŠ¥)
    laundering_score.loc[laundering_nodes["source"]] = 1
    laundering_score.loc[laundering_nodes["target"]] = 1

    print(f"âœ… Initialized laundering scores.")

    # ğŸš€ 6ï¸âƒ£ Propagation (CuPy í™œìš©í•œ ë³‘ë ¬ ì—°ì‚°)
    for i in range(max_iter):
        start = time.time()

        # ëª¨ë“  ë…¸ë“œì˜ ì´ì›ƒ í‰ê· ê°’ ì—…ë°ì´íŠ¸ (Jaccard Similarity ê¸°ë°˜)
        neighbors_df = cugraph.jaccard(G).to_pandas()
        neighbors_df["score"] = neighbors_df["jaccard_coeff"] * laundering_score.loc[neighbors_df["destination"]].values

        # ë…¸ë“œë³„ í‰ê·  ê³„ì‚°
        new_scores = neighbors_df.groupby("source")["score"].mean().to_dict()

        # ì—…ë°ì´íŠ¸
        laundering_score = laundering_score.to_pandas()
        laundering_score.update(new_scores)
        laundering_score = cudf.Series(laundering_score)

        end = time.time()
        print(f"âœ… Iteration {i + 1}: Label Propagation complete in {end - start:.5f} sec")

    # ğŸš€ 7ï¸âƒ£ Label Propagation ê²°ê³¼ë¥¼ ì›ë˜ `str` ê°’ìœ¼ë¡œ ë³€í™˜
    laundering_score_dict = laundering_score.to_pandas().to_dict()
    laundering_score_str = {inverse_node_map[node]: score for node, score in laundering_score_dict.items()}

    return laundering_score_str


if __name__ == "__main__":
    start = time.time()
    file_path = "../combined_dataset_v0.1.parquet"

    # GPU ê¸°ë°˜ Graph ìƒì„±
    G, gdf, node_map = create_gpu_graph(file_path)

    # Louvain + Label Propagation ìˆ˜í–‰
    laundering_score = propagation_laundering_score(G, gdf, node_map)

    # ì €ì¥
    with open("../laundering_score_cd_v0.1.json", "w") as f:
        json.dump(laundering_score, f)

    end = time.time()
    print(f"Completed all processes in {end - start:.5f} sec")
